{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d97de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It may take more than a few minutes to generate all the outputs. (for both Q1a and Q1b)\n",
    "## Your patience is greatly appreciated !!!!!\n",
    "## Thank you so much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff6e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://press.un.org/en']\n",
      "Crisis urls: 1\n",
      "https://press.un.org/en/2023/sgsm21982.doc.htm\n",
      "Crisis urls: 2\n",
      "https://press.un.org/en/2023/sgsm21980.doc.htm\n",
      "Crisis urls: 3\n",
      "https://press.un.org/en/2023/sgsm21978.doc.htm\n",
      "Crisis urls: 4\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
      "Crisis urls: 5\n",
      "https://press.un.org/en/2023/dsgsm1870.doc.htm\n",
      "Crisis urls: 6\n",
      "https://press.un.org/en/2023/sgsm21937.doc.htm\n",
      "Crisis urls: 7\n",
      "https://press.un.org/en/2023/sgsm21883.doc.htm\n",
      "Crisis urls: 8\n",
      "https://press.un.org/en/2023/sgsm21959.doc.htm\n",
      "Crisis urls: 9\n",
      "https://press.un.org/en/2023/sgsm21950.doc.htm\n",
      "Crisis urls: 10\n",
      "https://press.un.org/en/2023/sgsm21945.doc.htm\n"
     ]
    }
   ],
   "source": [
    "## Q1a\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#from urllib.request import Request\n",
    "\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "\n",
    "urls = [seed_url]    #queue of urls to crawl\n",
    "seen = [seed_url]    #stack of urls seen so far\n",
    "opened = []          #we keep track of seen urls so that we don't revisit them\n",
    "Crisis=[]\n",
    "\n",
    "########################################################################\n",
    "print(\"Starting with url=\"+str(urls))\n",
    "\n",
    "while len(urls) > 0 and len(Crisis) < 10:     \n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "       # print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "       # print(\"Trying to access= \"+curr_url)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "    except Exception as ex:\n",
    "       # print(\"Unable to access= \"+curr_url)\n",
    "       # print(ex)\n",
    "        continue    #skip code below\n",
    "########################################################################\n",
    "    \n",
    "    soup = BeautifulSoup(webpage)\n",
    "    for tag in soup.find_all('a', href=\"/en/press-release\"): #, hreflang=\"en\"):\n",
    "        text = soup.getText().lower()\n",
    "        if 'crisis' in text:\n",
    "            Crisis.append(curr_url)\n",
    "            print(\"Crisis urls: %d\" % len(Crisis))\n",
    "            print(curr_url)\n",
    "        \n",
    "########################################################################       \n",
    "    for tag in soup.find_all('a', href=True, hreflang=\"en\"): #find tags with links\n",
    "        childUrl = tag['href'] #extract just the link\n",
    "        o_childurl = childUrl\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        \n",
    "       \n",
    "        if seed_url in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9640a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "part_number=1\n",
    "\n",
    "for i in range(len(Crisis)):\n",
    "    html =requests.get(Crisis[i])\n",
    "    file = open(\"{}_{}.txt\".format(part_number,i+1), \"w\")\n",
    "    file.writelines(html.text)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "############################################################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3045509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.europarl.europa.eu/news/en/press-room']\n",
      "Crisis urls: 1\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acyzhu/anaconda3/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crisis urls: 2\n",
      "https://www.europarl.europa.eu/news/en/press-room/20221209IPR64426/eu-long-term-budget-needs-urgent-revision-to-cope-with-current-crises\n",
      "Crisis urls: 3\n",
      "https://www.europarl.europa.eu/news/en/press-room/20210304IPR99207/parliament-gives-green-light-for-new-eu4health-programme\n",
      "Crisis urls: 4\n",
      "https://www.europarl.europa.eu/news/en/press-room/20220909IPR40138/parliament-adopts-new-rules-on-adequate-minimum-wages-for-all-workers-in-the-eu\n",
      "Crisis urls: 5\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230310IPR77232/minimum-income-schemes-increasing-support-accessibility-and-inclusion\n",
      "Crisis urls: 6\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230210IPR74806/green-deal-industrial-plan-securing-the-eu-s-clean-tech-leadership\n",
      "Crisis urls: 7\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "Crisis urls: 8\n",
      "https://www.europarl.europa.eu/news/en/press-room/20220321IPR25913/more-eu-action-needed-for-secure-food-supply\n",
      "Crisis urls: 9\n",
      "https://www.europarl.europa.eu/news/en/press-room/20221209IPR64427/holodomor-parliament-recognises-soviet-starvation-of-ukrainians-as-genocide\n",
      "Crisis urls: 10\n",
      "https://www.europarl.europa.eu/news/en/press-room/20210422IPR02615/civil-protection-faster-eu-response-to-large-scale-emergencies\n"
     ]
    }
   ],
   "source": [
    "## Q1b\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#from urllib.request import Request\n",
    "\n",
    "seed_url = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "urls = [seed_url]    #queue of urls to crawl\n",
    "seen = [seed_url]    #stack of urls seen so far\n",
    "opened = []          #we keep track of seen urls so that we don't revisit them\n",
    "Crisis=[]\n",
    "\n",
    "########################################################################\n",
    "print(\"Starting with url=\"+str(urls))\n",
    "\n",
    "while len(urls) > 0 and len(Crisis) < 10:     \n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "           # print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "           # print(\"Trying to access= \"+curr_url)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "    except Exception as ex:\n",
    "        continue    #skip code below\n",
    "########################################################################\n",
    "    \n",
    "    soup = BeautifulSoup(webpage)\n",
    "    for tag in soup.find_all('span',class_=\"ep_name\"): #, hreflang=\"en\"):\n",
    "        ep_name=tag.text\n",
    "        if ep_name==\"Plenary session\" and soup.find('article',id= 'website-body'):\n",
    "            text = soup.getText().lower()\n",
    "            if 'crisis' in text:\n",
    "                Crisis.append(curr_url)\n",
    "                print(\"Crisis urls: %d\" % len(Crisis))\n",
    "                print(curr_url)\n",
    "        \n",
    "########################################################################       \n",
    "    for tag in soup.find_all('a', href=True): #find tags with links\n",
    "        childUrl = tag['href'] #extract just the link\n",
    "        o_childurl = childUrl\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "\n",
    "\n",
    "        if seed_url in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd66eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "part_number=2\n",
    "\n",
    "for i in range(len(Crisis)):\n",
    "    html =requests.get(Crisis[i])\n",
    "    file = open(\"{}_{}.txt\".format(part_number,i+1), \"w\")\n",
    "    file.writelines(html.text)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
